{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN CIFAR-10 Normalize Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ng [youtube](https://www.youtube.com/watch?v=FDCfw-YqWTE&index=9&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-07 at 12.49.17 PM.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centering\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\mu&=&\\frac{1}{m}\\sum_{i=1}^m{\\bf x}^{(i)}\\\\\n",
    "{\\bf x}^{(i)}&-=&\\mu\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardizing\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\sigma^2&=&\\frac{1}{m}\\sum_{i=1}^m{\\bf x}^{(i)}**2\\\\\n",
    "{\\bf x}^{(i)}&/=&\\sigma\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n",
      "\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n",
      "(50000,)\n",
      "(10000,)\n",
      "\n",
      "(32, 32, 3)\n",
      "(32, 32, 3)\n",
      "epoch_number    : 0\n",
      "cost_this_epoch : 703.3905563354492\n",
      "\n",
      "epoch_number    : 1\n",
      "cost_this_epoch : 481.24912345409393\n",
      "\n",
      "epoch_number    : 2\n",
      "cost_this_epoch : 385.63674914836884\n",
      "\n",
      "epoch_number    : 3\n",
      "cost_this_epoch : 306.9986826479435\n",
      "\n",
      "epoch_number    : 4\n",
      "cost_this_epoch : 233.5288823246956\n",
      "\n",
      "epoch_number    : 5\n",
      "cost_this_epoch : 162.51460602879524\n",
      "\n",
      "epoch_number    : 6\n",
      "cost_this_epoch : 104.21656459569931\n",
      "\n",
      "epoch_number    : 7\n",
      "cost_this_epoch : 64.78807900100946\n",
      "\n",
      "epoch_number    : 8\n",
      "cost_this_epoch : 50.834640227258205\n",
      "\n",
      "epoch_number    : 9\n",
      "cost_this_epoch : 38.25857106503099\n",
      "\n",
      "epoch_number    : 10\n",
      "cost_this_epoch : 35.77407338144258\n",
      "\n",
      "epoch_number    : 11\n",
      "cost_this_epoch : 33.37085730023682\n",
      "\n",
      "epoch_number    : 12\n",
      "cost_this_epoch : 31.466139998286963\n",
      "\n",
      "epoch_number    : 13\n",
      "cost_this_epoch : 28.48885112022981\n",
      "\n",
      "epoch_number    : 14\n",
      "cost_this_epoch : 24.021349710179493\n",
      "\n",
      "epoch_number    : 15\n",
      "cost_this_epoch : 27.29961344320327\n",
      "\n",
      "epoch_number    : 16\n",
      "cost_this_epoch : 28.658600056543946\n",
      "\n",
      "epoch_number    : 17\n",
      "cost_this_epoch : 26.348230478120968\n",
      "\n",
      "epoch_number    : 18\n",
      "cost_this_epoch : 20.44125392381102\n",
      "\n",
      "epoch_number    : 19\n",
      "cost_this_epoch : 19.094361297611613\n",
      "\n",
      "epoch_number    : 20\n",
      "cost_this_epoch : 20.298880709684454\n",
      "\n",
      "epoch_number    : 21\n",
      "cost_this_epoch : 25.518151313182898\n",
      "\n",
      "epoch_number    : 22\n",
      "cost_this_epoch : 18.04413326154463\n",
      "\n",
      "epoch_number    : 23\n",
      "cost_this_epoch : 14.483633915777318\n",
      "\n",
      "epoch_number    : 24\n",
      "cost_this_epoch : 24.721886939776596\n",
      "\n",
      "epoch_number    : 25\n",
      "cost_this_epoch : 19.309269761142787\n",
      "\n",
      "epoch_number    : 26\n",
      "cost_this_epoch : 17.156058437452884\n",
      "\n",
      "epoch_number    : 27\n",
      "cost_this_epoch : 17.721329072664957\n",
      "\n",
      "epoch_number    : 28\n",
      "cost_this_epoch : 18.016597278474364\n",
      "\n",
      "epoch_number    : 29\n",
      "cost_this_epoch : 16.812612134643132\n",
      "\n",
      "epoch_number    : 30\n",
      "cost_this_epoch : 15.407453739680932\n",
      "\n",
      "epoch_number    : 31\n",
      "cost_this_epoch : 16.569666174269514\n",
      "\n",
      "epoch_number    : 32\n",
      "cost_this_epoch : 20.221626795246266\n",
      "\n",
      "epoch_number    : 33\n",
      "cost_this_epoch : 17.232503360530245\n",
      "\n",
      "epoch_number    : 34\n",
      "cost_this_epoch : 14.238049510051496\n",
      "\n",
      "epoch_number    : 35\n",
      "cost_this_epoch : 14.140398815303342\n",
      "\n",
      "epoch_number    : 36\n",
      "cost_this_epoch : 18.281291990628233\n",
      "\n",
      "epoch_number    : 37\n",
      "cost_this_epoch : 17.915266483396408\n",
      "\n",
      "epoch_number    : 38\n",
      "cost_this_epoch : 12.820324285137758\n",
      "\n",
      "epoch_number    : 39\n",
      "cost_this_epoch : 12.481139044495649\n",
      "\n",
      "epoch_number    : 40\n",
      "cost_this_epoch : 17.701559024731978\n",
      "\n",
      "epoch_number    : 41\n",
      "cost_this_epoch : 15.630245741820545\n",
      "\n",
      "epoch_number    : 42\n",
      "cost_this_epoch : 14.768656980901142\n",
      "\n",
      "epoch_number    : 43\n",
      "cost_this_epoch : 13.57821446073649\n",
      "\n",
      "epoch_number    : 44\n",
      "cost_this_epoch : 15.651915566246316\n",
      "\n",
      "epoch_number    : 45\n",
      "cost_this_epoch : 15.252687785228773\n",
      "\n",
      "epoch_number    : 46\n",
      "cost_this_epoch : 15.279527123384469\n",
      "\n",
      "epoch_number    : 47\n",
      "cost_this_epoch : 13.435879358563398\n",
      "\n",
      "epoch_number    : 48\n",
      "cost_this_epoch : 14.848890008042872\n",
      "\n",
      "epoch_number    : 49\n",
      "cost_this_epoch : 15.631635139565333\n",
      "\n",
      "Test Accuracy:  0.4092\n",
      "[[597   0  76  46  76  10   5  26 159   5]\n",
      " [136 123  50 149  55  49  12  46 339  41]\n",
      " [118   0 295 188 204  83   8  46  53   5]\n",
      " [ 45   0  91 499 108 157  23  36  38   3]\n",
      " [ 54   0  72 169 560  54   7  51  32   1]\n",
      " [ 35   0  65 300 114 380  12  60  29   5]\n",
      " [ 23   0  86 294 208  80 255  20  33   1]\n",
      " [ 24   0  44 135 227 117   0 414  39   0]\n",
      " [ 98   0  23  34  38  10   1  17 776   3]\n",
      " [110   4  47 148  60  51   7  76 304 193]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# download these four fro https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "# we use these to download CIFAR10 dataset\n",
    "import cache    # module from Hvass Labs\n",
    "import dataset  # module from Hvass Labs\n",
    "import download # module from Hvass Labs\n",
    "import cifar10  # module from Hvass Labs\n",
    "\n",
    "import utils\n",
    "\n",
    "# this line should be commented out for regular python run \n",
    "%matplotlib inline  \n",
    "# this line should be commented out for regular python run \n",
    "\n",
    "\n",
    "\"\"\" Hyperparameter \"\"\"\n",
    "data_size_train = 50000\n",
    "data_size_test = 10000\n",
    "batch_size = 100\n",
    "lr = 1e-3\n",
    "epoch = 50\n",
    "\n",
    "\n",
    "\"\"\" Data Loading \"\"\"\n",
    "def load_cifar10():\n",
    "    # make directory if not exist\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    if not os.path.isdir(\"data/CIFAR-10\"):\n",
    "        os.mkdir(\"data/CIFAR-10\")\n",
    "\n",
    "    # download and extract if not done yet\n",
    "    # data is downloaded \n",
    "    # from data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    # to data_path  = \"data/CIFAR-10/\"\n",
    "    cifar10.data_path = \"data/CIFAR-10/\"\n",
    "    cifar10.maybe_download_and_extract()\n",
    "\n",
    "    # load data\n",
    "    x_train, y_train_cls, y_train = cifar10.load_training_data()\n",
    "    x_test, y_test_cls, y_test = cifar10.load_test_data()\n",
    "    class_names = cifar10.load_class_names()\n",
    "    \n",
    "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "    x_test  = (x_test - np.mean(x_train, axis=0))  / np.std(x_train, axis=0)\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train_cls = y_train_cls.astype(np.int32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test_cls = y_test_cls.astype(np.int32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    data = (x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names)\n",
    "    \n",
    "    print()\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    print(y_train_cls.shape)\n",
    "    print(y_test_cls.shape)\n",
    "    \n",
    "    print()\n",
    "    print(np.mean(x_train, axis=0).shape) # (32, 32, 3)\n",
    "    print(np.std(x_train, axis=0).shape) # (32, 32, 3)\n",
    "\n",
    "    return data\n",
    "\n",
    "# x_train.shape     :  (50000, 32, 32, 3)\n",
    "# x_test.shape      :  (10000, 32, 32, 3)\n",
    "# y_train.shape     :  (50000, 10)\n",
    "# y_test.shape      :  (10000, 10)\n",
    "# y_train_cls.shape :  (50000,)\n",
    "# y_test_cls.shape  :  (10000,)\n",
    "data = load_cifar10()\n",
    "x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names = data\n",
    "\n",
    "\n",
    "\"\"\" Graph Construction \"\"\"\n",
    "tf.random.set_random_seed(337)\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')\n",
    "y_cls = tf.placeholder(tf.int32, shape=(None,), name='y_cls')\n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# convolution layer 1\n",
    "# input and input.shape:   x,     (None, 32, 32, 3)\n",
    "# output and output.shape: conv1, (None, 16, 16, 32)\n",
    "conv1_W = tf.get_variable(\"conv1_W\", \\\n",
    "                          shape=(3,3,3,32), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "conv1 = tf.nn.conv2d(x, conv1_W, \\\n",
    "                     strides=(1,1,1,1), padding='SAME') # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.relu(conv1) # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 16, 16, 32)\n",
    "\n",
    "# convolution layer 2\n",
    "# input and input.shape:   conv1, (None, 16, 16, 32)\n",
    "# output and output.shape: conv2, (None, 8, 8, 64)\n",
    "conv2_W = tf.get_variable(\"conv2_W\", \\\n",
    "                          shape=(3,3,32,64), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "conv2 = tf.nn.conv2d(conv1, conv2_W, strides=(1,1,1,1), \\\n",
    "                     padding='SAME') # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.relu(conv2) # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 8, 8, 64)\n",
    "\n",
    "# fully connected layer\n",
    "# input and input.shape:   conv2, (None, 8, 8, 64)\n",
    "# output and output.shape: fc,    (None, 256) \n",
    "flatten = tf.reshape(conv2, (-1, 4096)) # (None, 4096) \n",
    "fc_W = tf.get_variable(\"fc_W\", \\\n",
    "                        shape=(4096,256), \\\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "fc = tf.matmul(flatten, fc_W) # (None, 256) \n",
    "fc = tf.nn.relu(fc) # (None, 256)\n",
    "\n",
    "# output layer\n",
    "# input and input.shape:   fc,     (None, 256) \n",
    "# output and output.shape: logits, (None, 10) \n",
    "out_W = tf.get_variable(\"out_W\", \\\n",
    "                        shape=(256, 10), \\\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "logits = fc @ out_W # (None, 10) \n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# y_pred, and y_pred_cls \n",
    "y_pred = tf.nn.softmax(logits, name='y_pred') # probabilities\n",
    "y_pred_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "# cross_entropy cost function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=y)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# optimizer\n",
    "# https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "# It is really important to get the update ops as stated in the Tensorflow documentation \n",
    "# because in training time the moving variance and the moving mean of the layer \n",
    "# have to be updated. \n",
    "# If you don’t do this, \n",
    "# batch normalization will not work and the network will not train as expected.\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "# test accuracy\n",
    "# you have to put test sets to compute test_accuracy\n",
    "correct_bool = tf.equal(y_cls, y_pred_cls)\n",
    "test_accuracy = tf.reduce_mean(tf.cast(correct_bool, tf.float32))\n",
    "\n",
    "\n",
    "\"\"\" Train and Test \"\"\"\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    # run gradient descent\n",
    "    for i in range(epoch):\n",
    "        if 1:\n",
    "            # perform random permutaion\n",
    "            idx = np.random.permutation(np.arange(data_size_train)) \n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "        else:\n",
    "            # don't perform random permutaion\n",
    "            x_batch = x_train\n",
    "            y_batch = y_train\n",
    "            \n",
    "        cost_this_epoch = 0\n",
    "        for batch_number in range(int(data_size_train/batch_size)): \n",
    "            x_b = x_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            y_b = y_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            feed_dict = {x: x_b, y: y_b}\n",
    "            _, cost_now = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "            cost_this_epoch += cost_now\n",
    "            \n",
    "        print('epoch_number    :', i)\n",
    "        print('cost_this_epoch :', cost_this_epoch)\n",
    "        print()\n",
    "\n",
    "            \n",
    "    # compute test accuracy and print confusion matrix \n",
    "    x_data = x_test\n",
    "    y_data = y_test\n",
    "    y_cls_data = y_test_cls\n",
    "    y_test_cls_pred = np.zeros(shape=(data_size_test), dtype=np.int32)\n",
    "    test_accuracy_list = []\n",
    "    for batch_number in range(int(data_size_test/batch_size)): \n",
    "        x_b = x_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_b = y_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_cls_b = y_cls_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        feed_dict = {x: x_b, y: y_b, y_cls: y_cls_b}\n",
    "        test_accuracy_temp, y_test_cls_pred_now = sess.run([test_accuracy,y_pred_cls],\n",
    "                                                           feed_dict=feed_dict)\n",
    "        test_accuracy_list.append(test_accuracy_temp)\n",
    "        y_test_cls_pred[batch_number*batch_size:(batch_number+1)*batch_size] = \\\n",
    "            y_test_cls_pred_now\n",
    "        \n",
    "    print('Test Accuracy: ', np.mean(np.array(test_accuracy_list)))\n",
    "    \n",
    "    cm = confusion_matrix(y_true=y_test_cls, y_pred=y_test_cls_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Input using Batch_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n",
      "epoch_number    : 0\n",
      "cost_this_epoch : 706.8615829348564\n",
      "\n",
      "epoch_number    : 1\n",
      "cost_this_epoch : 486.4096092581749\n",
      "\n",
      "epoch_number    : 2\n",
      "cost_this_epoch : 395.7249400615692\n",
      "\n",
      "epoch_number    : 3\n",
      "cost_this_epoch : 320.5073572397232\n",
      "\n",
      "epoch_number    : 4\n",
      "cost_this_epoch : 246.44318306446075\n",
      "\n",
      "epoch_number    : 5\n",
      "cost_this_epoch : 175.8351795077324\n",
      "\n",
      "epoch_number    : 6\n",
      "cost_this_epoch : 119.34511479735374\n",
      "\n",
      "epoch_number    : 7\n",
      "cost_this_epoch : 77.02031491324306\n",
      "\n",
      "epoch_number    : 8\n",
      "cost_this_epoch : 54.3723362069577\n",
      "\n",
      "epoch_number    : 9\n",
      "cost_this_epoch : 46.252362286671996\n",
      "\n",
      "epoch_number    : 10\n",
      "cost_this_epoch : 35.054065158590674\n",
      "\n",
      "epoch_number    : 11\n",
      "cost_this_epoch : 37.13487574690953\n",
      "\n",
      "epoch_number    : 12\n",
      "cost_this_epoch : 34.19042560970411\n",
      "\n",
      "epoch_number    : 13\n",
      "cost_this_epoch : 26.227373014669865\n",
      "\n",
      "epoch_number    : 14\n",
      "cost_this_epoch : 30.845078008249402\n",
      "\n",
      "epoch_number    : 15\n",
      "cost_this_epoch : 25.996311541181058\n",
      "\n",
      "epoch_number    : 16\n",
      "cost_this_epoch : 28.667955256532878\n",
      "\n",
      "epoch_number    : 17\n",
      "cost_this_epoch : 22.964820458786562\n",
      "\n",
      "epoch_number    : 18\n",
      "cost_this_epoch : 24.258775563677773\n",
      "\n",
      "epoch_number    : 19\n",
      "cost_this_epoch : 19.491131038521416\n",
      "\n",
      "epoch_number    : 20\n",
      "cost_this_epoch : 23.335272091673687\n",
      "\n",
      "epoch_number    : 21\n",
      "cost_this_epoch : 22.531008738209493\n",
      "\n",
      "epoch_number    : 22\n",
      "cost_this_epoch : 18.30570716544753\n",
      "\n",
      "epoch_number    : 23\n",
      "cost_this_epoch : 18.440878555265954\n",
      "\n",
      "epoch_number    : 24\n",
      "cost_this_epoch : 20.17897154111415\n",
      "\n",
      "epoch_number    : 25\n",
      "cost_this_epoch : 19.415374225296546\n",
      "\n",
      "epoch_number    : 26\n",
      "cost_this_epoch : 17.813403584441403\n",
      "\n",
      "epoch_number    : 27\n",
      "cost_this_epoch : 16.01817359426059\n",
      "\n",
      "epoch_number    : 28\n",
      "cost_this_epoch : 18.91765380767174\n",
      "\n",
      "epoch_number    : 29\n",
      "cost_this_epoch : 18.412539185315836\n",
      "\n",
      "epoch_number    : 30\n",
      "cost_this_epoch : 16.275954131720937\n",
      "\n",
      "epoch_number    : 31\n",
      "cost_this_epoch : 18.067084912225255\n",
      "\n",
      "epoch_number    : 32\n",
      "cost_this_epoch : 11.765279677696526\n",
      "\n",
      "epoch_number    : 33\n",
      "cost_this_epoch : 19.00698649842525\n",
      "\n",
      "epoch_number    : 34\n",
      "cost_this_epoch : 15.357230902081938\n",
      "\n",
      "epoch_number    : 35\n",
      "cost_this_epoch : 9.76300108016585\n",
      "\n",
      "epoch_number    : 36\n",
      "cost_this_epoch : 12.683206457193592\n",
      "\n",
      "epoch_number    : 37\n",
      "cost_this_epoch : 15.882317490671994\n",
      "\n",
      "epoch_number    : 38\n",
      "cost_this_epoch : 14.927450742172368\n",
      "\n",
      "epoch_number    : 39\n",
      "cost_this_epoch : 13.084174801406334\n",
      "\n",
      "epoch_number    : 40\n",
      "cost_this_epoch : 13.493664406931202\n",
      "\n",
      "epoch_number    : 41\n",
      "cost_this_epoch : 14.703892257748521\n",
      "\n",
      "epoch_number    : 42\n",
      "cost_this_epoch : 11.935610941021878\n",
      "\n",
      "epoch_number    : 43\n",
      "cost_this_epoch : 13.068691081694851\n",
      "\n",
      "epoch_number    : 44\n",
      "cost_this_epoch : 10.111784754364635\n",
      "\n",
      "epoch_number    : 45\n",
      "cost_this_epoch : 14.987408540073375\n",
      "\n",
      "epoch_number    : 46\n",
      "cost_this_epoch : 13.322858441424614\n",
      "\n",
      "epoch_number    : 47\n",
      "cost_this_epoch : 9.48478011260886\n",
      "\n",
      "epoch_number    : 48\n",
      "cost_this_epoch : 13.205557942317682\n",
      "\n",
      "epoch_number    : 49\n",
      "cost_this_epoch : 11.776667061938497\n",
      "\n",
      "Test Accuracy:  0.7051\n",
      "[[753  27  67  24  25   8  10  15  46  25]\n",
      " [ 26 825   7  13   6   4  11   4  29  75]\n",
      " [ 59   4 593  90  85  53  71  26  13   6]\n",
      " [ 14   6  84 539  63 163  76  31  12  12]\n",
      " [ 19   6  62  71 684  39  48  59   7   5]\n",
      " [ 10   6  45 197  51 606  26  41   5  13]\n",
      " [  2   5  40  69  46  31 789   8   7   3]\n",
      " [ 19   4  44  60  60  51   8 734   9  11]\n",
      " [ 79  39  23  14  16  12  13   4 782  18]\n",
      " [ 32 118  16  18   8   6  12  19  25 746]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cache    # module from Hvass Labs\n",
    "import dataset  # module from Hvass Labs\n",
    "import download # module from Hvass Labs\n",
    "import cifar10  # module from Hvass Labs\n",
    "\n",
    "import utils\n",
    "\n",
    "# this line should be commented out for regular python run \n",
    "%matplotlib inline  \n",
    "# this line should be commented out for regular python run \n",
    "\n",
    "\n",
    "\"\"\" Hyperparameter \"\"\"\n",
    "data_size_train = 50000\n",
    "data_size_test = 10000\n",
    "batch_size = 100\n",
    "lr = 1e-3\n",
    "epoch = 50\n",
    "\n",
    "\n",
    "\"\"\" Data Loading \"\"\"\n",
    "def load_cifar10():\n",
    "    # make directory if not exist\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    if not os.path.isdir(\"data/CIFAR-10\"):\n",
    "        os.mkdir(\"data/CIFAR-10\")\n",
    "\n",
    "    # download and extract if not done yet\n",
    "    # data is downloaded \n",
    "    # from data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    # to data_path  = \"data/CIFAR-10/\"\n",
    "    cifar10.data_path = \"data/CIFAR-10/\"\n",
    "    cifar10.maybe_download_and_extract()\n",
    "\n",
    "    # load data\n",
    "    x_train, y_train_cls, y_train = cifar10.load_training_data()\n",
    "    x_test, y_test_cls, y_test = cifar10.load_test_data()\n",
    "    class_names = cifar10.load_class_names()\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train_cls = y_train_cls.astype(np.int32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test_cls = y_test_cls.astype(np.int32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    data = (x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names)\n",
    "\n",
    "    return data\n",
    "\n",
    "# x_train.shape     :  (50000, 32, 32, 3)\n",
    "# x_test.shape      :  (10000, 32, 32, 3)\n",
    "# y_train.shape     :  (50000, 10)\n",
    "# y_test.shape      :  (10000, 10)\n",
    "# y_train_cls.shape :  (50000,)\n",
    "# y_test_cls.shape  :  (10000,)\n",
    "data = load_cifar10()\n",
    "x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names = data\n",
    "\n",
    "\n",
    "\"\"\" Graph Construction \"\"\"\n",
    "tf.random.set_random_seed(337)\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')\n",
    "y_cls = tf.placeholder(tf.int32, shape=(None,), name='y_cls')\n",
    "is_train = tf.placeholder(tf.bool, shape=(), name='is_train')\n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# convolution layer 1\n",
    "# input and input.shape:   x,     (None, 32, 32, 3)\n",
    "# output and output.shape: conv1, (None, 16, 16, 32)\n",
    "x_normalized = tf.layers.batch_normalization(x, training=is_train)\n",
    "conv1_W = tf.get_variable(\"conv1_W\", \\\n",
    "                          shape=(3,3,3,32), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "conv1 = tf.nn.conv2d(x_normalized, conv1_W, \\\n",
    "                     strides=(1,1,1,1), padding='SAME') # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.relu(conv1) # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 16, 16, 32)\n",
    "\n",
    "# convolution layer 2\n",
    "# input and input.shape:   conv1, (None, 16, 16, 32)\n",
    "# output and output.shape: conv2, (None, 8, 8, 64)\n",
    "conv2_W = tf.get_variable(\"conv2_W\", \\\n",
    "                          shape=(3,3,32,64), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "conv2 = tf.nn.conv2d(conv1, conv2_W, strides=(1,1,1,1), \\\n",
    "                     padding='SAME') # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.relu(conv2) # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 8, 8, 64)\n",
    "\n",
    "# fully connected layer\n",
    "# input and input.shape:   conv2, (None, 8, 8, 64)\n",
    "# output and output.shape: fc,    (None, 256) \n",
    "flatten = tf.reshape(conv2, (-1, 4096)) # (None, 4096) \n",
    "fc_W = tf.get_variable(\"fc_W\", \\\n",
    "                        shape=(4096,256), \\\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "fc = tf.matmul(flatten, fc_W) # (None, 256) \n",
    "fc = tf.nn.relu(fc) # (None, 256)\n",
    "\n",
    "# output layer\n",
    "# input and input.shape:   fc,     (None, 256) \n",
    "# output and output.shape: logits, (None, 10) \n",
    "out_W = tf.get_variable(\"out_W\", \\\n",
    "                        shape=(256, 10), \\\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "logits = fc @ out_W # (None, 10) \n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# y_pred, and y_pred_cls \n",
    "y_pred = tf.nn.softmax(logits, name='y_pred') # probabilities\n",
    "y_pred_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "# cross_entropy cost function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=y)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# optimizer\n",
    "# https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "# It is really important to get the update ops as stated in the Tensorflow documentation \n",
    "# because in training time the moving variance and the moving mean of the layer \n",
    "# have to be updated. \n",
    "# If you don’t do this, \n",
    "# batch normalization will not work and the network will not train as expected.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "# test accuracy\n",
    "# you have to put test sets to compute test_accuracy\n",
    "correct_bool = tf.equal(y_cls, y_pred_cls)\n",
    "test_accuracy = tf.reduce_mean(tf.cast(correct_bool, tf.float32))\n",
    "\n",
    "\n",
    "\"\"\" Train and Test \"\"\"\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    # run gradient descent\n",
    "    for i in range(epoch):\n",
    "        if 1:\n",
    "            # perform random permutaion\n",
    "            idx = np.random.permutation(np.arange(data_size_train)) \n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "        else:\n",
    "            # don't perform random permutaion\n",
    "            x_batch = x_train\n",
    "            y_batch = y_train\n",
    "            \n",
    "        cost_this_epoch = 0\n",
    "        for batch_number in range(int(data_size_train/batch_size)): \n",
    "            x_b = x_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            y_b = y_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            feed_dict = {x: x_b, y: y_b, is_train: True}\n",
    "            _, cost_now = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "            cost_this_epoch += cost_now\n",
    "            \n",
    "        print('epoch_number    :', i)\n",
    "        print('cost_this_epoch :', cost_this_epoch)\n",
    "        print()\n",
    "\n",
    "            \n",
    "    # compute test accuracy and print confusion matrix \n",
    "    x_data = x_test\n",
    "    y_data = y_test\n",
    "    y_cls_data = y_test_cls\n",
    "    y_test_cls_pred = np.zeros(shape=(data_size_test), dtype=np.int32)\n",
    "    test_accuracy_list = []\n",
    "    for batch_number in range(int(data_size_test/batch_size)): \n",
    "        x_b = x_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_b = y_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_cls_b = y_cls_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        feed_dict = {x: x_b, y: y_b, y_cls: y_cls_b, is_train: False}\n",
    "        test_accuracy_temp, y_test_cls_pred_now = sess.run([test_accuracy,y_pred_cls],\n",
    "                                                           feed_dict=feed_dict)\n",
    "        test_accuracy_list.append(test_accuracy_temp)\n",
    "        y_test_cls_pred[batch_number*batch_size:(batch_number+1)*batch_size] = \\\n",
    "            y_test_cls_pred_now\n",
    "        \n",
    "    print('Test Accuracy: ', np.mean(np.array(test_accuracy_list)))\n",
    "    \n",
    "    cm = confusion_matrix(y_true=y_test_cls, y_pred=y_test_cls_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
