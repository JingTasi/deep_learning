{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN CIFAR-10 Initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the default kernel initializer in tf.layers.conv2d and tf.layers.dense?\n",
    "\n",
    "So the answer is: it uses the glorot_uniform_initializer. - rmeertens\n",
    "\n",
    "The Glorot uniform initializer, also called Xavier uniform initializer:\n",
    "$$\n",
    "\\mbox{Glorot_uniform_initializer} \n",
    "= \\mbox{Uniform}\\left(-\\mbox{limit}, \\mbox{limit}\\right),\\quad\n",
    "\\mbox{limit} = \\sqrt{\\frac{6}{\\mbox{fan_in} + \\mbox{fan_out}}}\n",
    "$$\n",
    "https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer\n",
    "\n",
    "The Glorot normal initializer, also called Xavier normal initializer:\n",
    "$$\n",
    "\\mbox{Glorot_normal_initializer} \n",
    "= \\mbox{Truncated_normal}\\left(0,\\sigma^2\\right),\\quad\n",
    "\\sigma = \\sqrt{\\frac{2}{\\mbox{fan_in} + \\mbox{fan_out}}}\\quad\\mbox{NOP!!!}\n",
    "$$\n",
    "$$$$\n",
    "$$\n",
    "\\mbox{Glorot_normal_initializer} \n",
    "= \\mbox{Truncated_normal}\\left(0,\\sigma^2\\right),\\quad\n",
    "\\sigma = \\sqrt{\\frac{2}{\\mbox{fan_in} + \\mbox{fan_out}}} / .87962566103423978\n",
    "$$\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\n",
    "\n",
    "According to this course by Andrew Ng and the Xavier documentation, \n",
    "if you are using ReLU as activation function, \n",
    "better change the default weights initializer(which is Xavier uniform) to Xavier normal - xtluo\n",
    "\n",
    "https://stackoverflow.com/questions/43284047/what-is-the-default-kernel-initializer-in-tf-layers-conv2d-and-tf-layers-dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n",
      "epoch_number    : 0\n",
      "cost_this_epoch : 709.6140133142471\n",
      "\n",
      "epoch_number    : 1\n",
      "cost_this_epoch : 528.1044263243675\n",
      "\n",
      "epoch_number    : 2\n",
      "cost_this_epoch : 454.2781323194504\n",
      "\n",
      "epoch_number    : 3\n",
      "cost_this_epoch : 401.991040289402\n",
      "\n",
      "epoch_number    : 4\n",
      "cost_this_epoch : 356.0044216811657\n",
      "\n",
      "epoch_number    : 5\n",
      "cost_this_epoch : 313.65453296899796\n",
      "\n",
      "epoch_number    : 6\n",
      "cost_this_epoch : 273.6703358888626\n",
      "\n",
      "epoch_number    : 7\n",
      "cost_this_epoch : 235.0086189508438\n",
      "\n",
      "epoch_number    : 8\n",
      "cost_this_epoch : 197.01153506338596\n",
      "\n",
      "epoch_number    : 9\n",
      "cost_this_epoch : 159.9681484401226\n",
      "\n",
      "epoch_number    : 10\n",
      "cost_this_epoch : 128.3562091961503\n",
      "\n",
      "epoch_number    : 11\n",
      "cost_this_epoch : 103.2494084239006\n",
      "\n",
      "epoch_number    : 12\n",
      "cost_this_epoch : 85.18833726271987\n",
      "\n",
      "epoch_number    : 13\n",
      "cost_this_epoch : 64.35954350978136\n",
      "\n",
      "epoch_number    : 14\n",
      "cost_this_epoch : 53.15085278078914\n",
      "\n",
      "Test Accuracy:  0.7057999\n",
      "[[721   9  49  22  17   9  17  13  88  55]\n",
      " [ 37 727   4   9   5   5  18   5  53 137]\n",
      " [ 67   1 623  59  59  72  56  36  19   8]\n",
      " [ 25   7  70 534  46 194  59  35  11  19]\n",
      " [ 18   3  98  84 602  53  52  73  14   3]\n",
      " [ 16   1  54 174  41 611  27  63   5   8]\n",
      " [  4   2  60  66  33  25 788   7   7   8]\n",
      " [ 17   5  40  32  50  50   6 781   5  14]\n",
      " [ 47  18  20  15   8   8   4   5 841  34]\n",
      " [ 27  39  19  13   4   8  10  16  34 830]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cache    # module from Hvass Labs\n",
    "import dataset  # module from Hvass Labs\n",
    "import download # module from Hvass Labs\n",
    "import cifar10  # module from Hvass Labs\n",
    "\n",
    "import utils\n",
    "\n",
    "# this line should be commented out for regular python run \n",
    "%matplotlib inline  \n",
    "# this line should be commented out for regular python run \n",
    "\n",
    "\n",
    "\"\"\" Hyperparameter \"\"\"\n",
    "data_size_train = 50000\n",
    "data_size_test = 10000\n",
    "batch_size = 100\n",
    "lr = 1e-3\n",
    "epoch = 15\n",
    "\n",
    "\n",
    "\"\"\" Data Loading \"\"\"\n",
    "def load_cifar10():\n",
    "    # make directory if not exist\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    if not os.path.isdir(\"data/CIFAR-10\"):\n",
    "        os.mkdir(\"data/CIFAR-10\")\n",
    "\n",
    "    # download and extract if not done yet\n",
    "    # data is downloaded \n",
    "    # from data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    # to data_path  = \"data/CIFAR-10/\"\n",
    "    cifar10.data_path = \"data/CIFAR-10/\"\n",
    "    cifar10.maybe_download_and_extract()\n",
    "\n",
    "    # load data\n",
    "    x_train, y_train_cls, y_train = cifar10.load_training_data()\n",
    "    x_test, y_test_cls, y_test = cifar10.load_test_data()\n",
    "    class_names = cifar10.load_class_names()\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train_cls = y_train_cls.astype(np.int32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test_cls = y_test_cls.astype(np.int32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    data = (x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names)\n",
    "\n",
    "    return data\n",
    "\n",
    "# x_train.shape     :  (50000, 32, 32, 3)\n",
    "# x_test.shape      :  (10000, 32, 32, 3)\n",
    "# y_train.shape     :  (50000, 10)\n",
    "# y_test.shape      :  (10000, 10)\n",
    "# y_train_cls.shape :  (50000,)\n",
    "# y_test_cls.shape  :  (10000,)\n",
    "data = load_cifar10()\n",
    "x_train, y_train_cls, y_train, x_test, y_test_cls, y_test, class_names = data\n",
    "\n",
    "\n",
    "\"\"\" Graph Construction \"\"\"\n",
    "tf.random.set_random_seed(337)\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')\n",
    "y_cls = tf.placeholder(tf.int32, shape=(None,), name='y_cls')\n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# convolution layer 1\n",
    "# input and input.shape:   x,     (None, 32, 32, 3)\n",
    "# output and output.shape: conv1, (None, 16, 16, 32)\n",
    "s1 = np.sqrt(2 / (3*3*3 + 3*3*32)) / .87962566103423978\n",
    "conv1_W = tf.get_variable(\"conv1_W\", \\\n",
    "                          shape=(3,3,3,32), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=s1))\n",
    "conv1 = tf.nn.conv2d(x, conv1_W, strides=(1,1,1,1), padding='SAME') # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.relu(conv1) # (None, 32, 32, 32)\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 16, 16, 32)\n",
    "\n",
    "# convolution layer 2\n",
    "# input and input.shape:   conv1, (None, 16, 16, 32)\n",
    "# output and output.shape: conv2, (None, 8, 8, 64)\n",
    "s2 = np.sqrt(2 / (3*3*32 + 3*3*64)) / .87962566103423978\n",
    "conv2_W = tf.get_variable(\"conv2_W\", \\\n",
    "                          shape=(3,3,32,64), \\\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=s2))\n",
    "conv2 = tf.nn.conv2d(conv1, conv2_W, strides=(1,1,1,1), \\\n",
    "                     padding='SAME') # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.relu(conv2) # (None, 16, 16, 64)\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=(1,2,2,1), strides=(1,2,2,1), \\\n",
    "                       padding='SAME') # (None, 8, 8, 64)\n",
    "\n",
    "# fully connected layer\n",
    "# input and input.shape:   conv2, (None, 8, 8, 64)\n",
    "# output and output.shape: fc,    (None, 256) \n",
    "flatten = tf.reshape(conv2, (-1, 4096)) # (None, 4096) \n",
    "s3 = np.sqrt(2 / (4096 + 256)) / .87962566103423978\n",
    "fc_W = tf.get_variable(\"fc_W\", \\\n",
    "                        shape=(4096,256), \\\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=s3))\n",
    "fc = tf.matmul(flatten, fc_W) # (None, 256) \n",
    "fc = tf.nn.relu(fc) # (None, 256) \n",
    "\n",
    "# output layer\n",
    "# input and input.shape:   fc,     (None, 256) \n",
    "# output and output.shape: logits, (None, 10) \n",
    "out_W = tf.get_variable(\"out_W\", \\\n",
    "                        shape=(256, 10), \\\n",
    "                        initializer=tf.keras.initializers.glorot_uniform)\n",
    "logits = fc @ out_W # (None, 10) \n",
    "\n",
    "# weights and layers #################################################################\n",
    "\n",
    "# y_pred, and y_pred_cls \n",
    "y_pred = tf.nn.softmax(logits, name='y_pred') # probabilities\n",
    "y_pred_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "# cross_entropy cost function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=y)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "# test accuracy\n",
    "# you have to put test sets to compute test_accuracy\n",
    "correct_bool = tf.equal(y_cls, y_pred_cls)\n",
    "test_accuracy = tf.reduce_mean(tf.cast(correct_bool, tf.float32))\n",
    "\n",
    "\n",
    "\"\"\" Train and Test \"\"\"\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    # run gradient descent\n",
    "    for i in range(epoch):\n",
    "        if 1:\n",
    "            # perform random permutaion\n",
    "            idx = np.random.permutation(np.arange(data_size_train)) \n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "        else:\n",
    "            # don't perform random permutaion\n",
    "            x_batch = x_train\n",
    "            y_batch = y_train\n",
    "            \n",
    "        cost_this_epoch = 0\n",
    "        for batch_number in range(int(data_size_train/batch_size)): \n",
    "            x_b = x_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            y_b = y_batch[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "            feed_dict = {x: x_b, y: y_b}\n",
    "            _, cost_now = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "            cost_this_epoch += cost_now\n",
    "            \n",
    "        print('epoch_number    :', i)\n",
    "        print('cost_this_epoch :', cost_this_epoch)\n",
    "        print()\n",
    "\n",
    "            \n",
    "    # compute test accuracy and print confusion matrix \n",
    "    x_data = x_test\n",
    "    y_data = y_test\n",
    "    y_cls_data = y_test_cls\n",
    "    y_test_cls_pred = np.zeros(shape=(data_size_test), dtype=np.int32)\n",
    "    test_accuracy_list = []\n",
    "    for batch_number in range(int(data_size_test/batch_size)): \n",
    "        x_b = x_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_b = y_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        y_cls_b = y_cls_data[batch_number*batch_size:(batch_number+1)*batch_size]\n",
    "        feed_dict = {x: x_b, y: y_b, y_cls: y_cls_b}\n",
    "        test_accuracy_temp, y_test_cls_pred_now = sess.run([test_accuracy,y_pred_cls],\n",
    "                                                           feed_dict=feed_dict)\n",
    "        test_accuracy_list.append(test_accuracy_temp)\n",
    "        y_test_cls_pred[batch_number*batch_size:(batch_number+1)*batch_size] = \\\n",
    "            y_test_cls_pred_now\n",
    "        \n",
    "    print('Test Accuracy: ', np.mean(np.array(test_accuracy_list)))\n",
    "    \n",
    "    cm = confusion_matrix(y_true=y_test_cls, y_pred=y_test_cls_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
